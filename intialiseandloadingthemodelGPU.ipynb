{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm","authorship_tag":"ABX9TyPUsyXvp5PLe7LccKhG/s1m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","\n","# -------------------------------\n","# LLAMA32 Configuration\n","# -------------------------------\n","LLAMA32_CONFIG = {\n","    \"vocab_size\": 128_256,\n","    \"context_length\": 131_072,  # max context\n","    \"emb_dim\": 3072,\n","    \"n_heads\": 24,\n","    \"n_layers\": 28,\n","    \"hidden_dim\": 8192,\n","    \"n_kv_groups\": 8,\n","    \"rope_base\": 500_000.0,\n","    \"dtype\": torch.bfloat16,\n","    \"rope_freq\": {\n","        \"factor\": 32.0,\n","        \"low_freq_factor\": 1.0,\n","        \"high_freq_factor\": 4.0,\n","        \"original_context_length\": 8192,\n","    }\n","}\n","\n","# -------------------------------\n","# Rotary Positional Encoding (RoPE)\n","# -------------------------------\n","def apply_rope(x, base=500_000.0):\n","    batch, seq_len, dim = x.shape\n","    half_dim = dim // 2\n","    freq_seq = torch.arange(half_dim, dtype=x.dtype, device=x.device)\n","    freq_seq = 1.0 / (base ** (freq_seq / half_dim))\n","    positions = torch.arange(seq_len, dtype=x.dtype, device=x.device)\n","    angles = torch.einsum(\"i,j->ij\", positions, freq_seq)\n","    cos = torch.cos(angles)\n","    sin = torch.sin(angles)\n","    x1, x2 = x[..., :half_dim], x[..., half_dim:]\n","    return torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n","\n","# -------------------------------\n","# Multi-Head Attention with KV cache and causal masking\n","# -------------------------------\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.emb_dim = config[\"emb_dim\"]\n","        self.n_heads = config[\"n_heads\"]\n","        self.head_dim = self.emb_dim // self.n_heads\n","        self.qkv_proj = nn.Linear(self.emb_dim, self.emb_dim * 3)\n","        self.out_proj = nn.Linear(self.emb_dim, self.emb_dim)\n","\n","    def forward(self, x, kv_cache=None):\n","        batch, seq_len, emb_dim = x.shape\n","\n","        # Apply Rotary Positional Encoding\n","        x = apply_rope(x, base=LLAMA32_CONFIG[\"rope_base\"])\n","\n","        # Compute QKV\n","        qkv = self.qkv_proj(x)\n","        qkv = qkv.view(batch, seq_len, 3, self.n_heads, self.head_dim)\n","        q, k, v = qkv[:, :, 0], qkv[:, :, 1], qkv[:, :, 2]\n","\n","        # Move heads forward\n","        q = q.transpose(1, 2)  # [batch, heads, seq_len, head_dim]\n","        k = k.transpose(1, 2)\n","        v = v.transpose(1, 2)\n","\n","        # Append to KV cache\n","        if kv_cache is not None:\n","            k = torch.cat([kv_cache[\"k\"], k], dim=2)\n","            v = torch.cat([kv_cache[\"v\"], v], dim=2)\n","        kv_cache = {\"k\": k, \"v\": v}\n","\n","        # Causal mask\n","        seq_len_total = k.size(2)\n","        mask = torch.tril(torch.ones(seq_len_total, seq_len_total, device=x.device)).unsqueeze(0).unsqueeze(0)\n","        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n","        attn_scores = attn_scores.masked_fill(mask[:, :, -seq_len:, :] == 0, float('-inf'))\n","\n","        # Attention probabilities\n","        attn_probs = F.softmax(attn_scores, dim=-1)\n","        out = torch.matmul(attn_probs, v)\n","\n","        # Merge heads\n","        out = out.transpose(1, 2).reshape(batch, seq_len, emb_dim)\n","        out = self.out_proj(out)\n","        return out, kv_cache\n","\n","# -------------------------------\n","# Feedforward network\n","# -------------------------------\n","class FeedForward(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.fc1 = nn.Linear(config[\"emb_dim\"], config[\"hidden_dim\"])\n","        self.activation = nn.GELU()\n","        self.fc2 = nn.Linear(config[\"hidden_dim\"], config[\"emb_dim\"])\n","\n","    def forward(self, x):\n","        return self.fc2(self.activation(self.fc1(x)))\n","\n","# -------------------------------\n","# Transformer block\n","# -------------------------------\n","class TransformerBlock(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.attn = MultiHeadAttention(config)\n","        self.ffn = FeedForward(config)\n","        self.ln1 = nn.LayerNorm(config[\"emb_dim\"], eps=1e-5)\n","        self.ln2 = nn.LayerNorm(config[\"emb_dim\"], eps=1e-5)\n","\n","    def forward(self, x, kv_cache=None):\n","        attn_out, kv_cache = self.attn(self.ln1(x), kv_cache)\n","        x = x + attn_out\n","        x = x + self.ffn(self.ln2(x))\n","        return x, kv_cache\n","\n","# -------------------------------\n","# Full LLAMA32 Model\n","# -------------------------------\n","class LLAMA32(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        self.token_emb = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n","        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(config[\"n_layers\"])])\n","        self.ln_final = nn.LayerNorm(config[\"emb_dim\"], eps=1e-5)\n","        self.head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n","\n","    def forward(self, input_ids, kv_caches=None):\n","        x = self.token_emb(input_ids)\n","        new_caches = []\n","        for i, block in enumerate(self.blocks):\n","            kv_cache = None if kv_caches is None else kv_caches[i]\n","            x, cache = block(x, kv_cache)\n","            new_caches.append(cache)\n","        x = self.ln_final(x)\n","        logits = self.head(x)\n","        return logits, new_caches\n","\n","# -------------------------------\n","# Testing\n","# -------------------------------\n","if __name__ == \"__main__\":\n","    # Device setup\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(f\"Using device: {device}\")\n","\n","    # Reduce context length for testing\n","    test_context_length = 64\n","    cfg = LLAMA32_CONFIG.copy()\n","    cfg[\"context_length\"] = test_context_length\n","\n","    # Initialize model\n","    model = LLAMA32(cfg).to(device=device, dtype=cfg[\"dtype\"])\n","    model.eval()\n","\n","    # Dummy input\n","    batch_size = 2\n","    seq_len = test_context_length\n","    dummy_input_ids = torch.randint(0, cfg[\"vocab_size\"], (batch_size, seq_len), device=device)\n","\n","    # Forward pass\n","    with torch.no_grad():\n","        logits, caches = model(dummy_input_ids)\n","\n","    # Output shapes\n","    print(\"Logits shape:\", logits.shape)  # [batch, seq_len, vocab_size]\n","    print(\"KV cache for first block k shape:\", caches[0][\"k\"].shape)  # [batch, heads, seq_len, head_dim]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qoyOYeM-3gu_","executionInfo":{"status":"ok","timestamp":1769001286398,"user_tz":0,"elapsed":29977,"user":{"displayName":"Ayeni Oluwatosin Olawale","userId":"15779955476633171344"}},"outputId":"84e33f79-7c22-4dc6-f23d-5a9c43c76daa"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Logits shape: torch.Size([2, 64, 128256])\n","KV cache for first block k shape: torch.Size([2, 24, 64, 128])\n"]}]}]}