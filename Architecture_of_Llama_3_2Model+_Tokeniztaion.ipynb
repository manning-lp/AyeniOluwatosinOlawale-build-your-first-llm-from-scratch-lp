{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VTHNcQ6B6M6C"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RMSNorm (Llama uses RMSNorm)"
      ],
      "metadata": {
        "id": "QWgH-8uj97uX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- RMSNorm ---\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        norm = x.pow(2).mean(-1, keepdim=True)\n",
        "        return x * torch.rsqrt(norm + self.eps) * self.weight\n"
      ],
      "metadata": {
        "id": "cR5u0RcP9vVv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rotary Positional Embeddings (RoPE)"
      ],
      "metadata": {
        "id": "tS26KTHS-A8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- RoPE ---\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim, base=500_000):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.base = base\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
        "    def forward(self, seq_len, device, dtype):\n",
        "        t = torch.arange(seq_len, device=device, dtype=dtype)\n",
        "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq.to(dtype))\n",
        "        return torch.cos(freqs), torch.sin(freqs)\n",
        "\n",
        "def apply_rope(x, cos, sin):\n",
        "    cos = cos.to(x.dtype)\n",
        "    sin = sin.to(x.dtype)\n",
        "    x1, x2 = x[..., ::2], x[..., 1::2]\n",
        "    return torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n"
      ],
      "metadata": {
        "id": "uoHw6fdf-M3g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama Feed-Forward Network (SwiGLU)"
      ],
      "metadata": {
        "id": "di8nBN4KCptJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FeedForward ---\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.gate = nn.Linear(config[\"emb_dim\"], config[\"hidden_dim\"], bias=False)\n",
        "        self.up = nn.Linear(config[\"emb_dim\"], config[\"hidden_dim\"], bias=False)\n",
        "        self.down = nn.Linear(config[\"hidden_dim\"], config[\"emb_dim\"], bias=False)\n",
        "    def forward(self, x):\n",
        "        return self.down(F.silu(self.gate(x)) * self.up(x))"
      ],
      "metadata": {
        "id": "yH37m3WICn_e"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grouped-Query Multi-Head Attention (GQA)"
      ],
      "metadata": {
        "id": "EtmYeFc2-UnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MultiHeadAttention ---\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.emb_dim = config[\"emb_dim\"]\n",
        "        self.n_heads = config[\"n_heads\"]\n",
        "        self.n_kv = config[\"n_kv_groups\"]\n",
        "        self.head_dim = self.emb_dim // self.n_heads\n",
        "\n",
        "        self.q_proj = nn.Linear(self.emb_dim, self.emb_dim, bias=False)\n",
        "        self.k_proj = nn.Linear(self.emb_dim, self.n_kv * self.head_dim, bias=False)\n",
        "        self.v_proj = nn.Linear(self.emb_dim, self.n_kv * self.head_dim, bias=False)\n",
        "        self.o_proj = nn.Linear(self.emb_dim, self.emb_dim, bias=False)\n",
        "\n",
        "        self.rope = RotaryEmbedding(self.head_dim, base=config[\"rope_base\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        dtype = x.dtype\n",
        "\n",
        "        # Project QKV\n",
        "        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim)\n",
        "        k = self.k_proj(x).view(B, T, self.n_kv, self.head_dim)\n",
        "        v = self.v_proj(x).view(B, T, self.n_kv, self.head_dim)\n",
        "\n",
        "        # RoPE\n",
        "        cos, sin = self.rope(T, x.device, dtype)\n",
        "        cos, sin = cos[None, :, None, :], sin[None, :, None, :]\n",
        "        q = apply_rope(q, cos, sin)\n",
        "        k = apply_rope(k, cos, sin)\n",
        "\n",
        "        # Expand KV for grouped query\n",
        "        k = k.repeat_interleave(self.n_heads // self.n_kv, dim=2)\n",
        "        v = v.repeat_interleave(self.n_heads // self.n_kv, dim=2)\n",
        "\n",
        "        # Transpose for attention\n",
        "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
        "\n",
        "        # Attention scores\n",
        "        attn_scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # Causal mask\n",
        "        causal_mask = torch.tril(torch.ones(T, T, device=x.device, dtype=torch.bool))\n",
        "        attn_scores = attn_scores.masked_fill(~causal_mask, float(\"-inf\"))\n",
        "\n",
        "        # Softmax & output\n",
        "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "        out = attn_probs @ v\n",
        "        out = out.transpose(1,2).contiguous().view(B, T, C)\n",
        "        return self.o_proj(out)"
      ],
      "metadata": {
        "id": "oUfTaxNw-dex"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfomer Blocks ‚Üí Llama 3.2 Model"
      ],
      "metadata": {
        "id": "tqfQ-DFh_yf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Transformer Block ---\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attn_norm = RMSNorm(config[\"emb_dim\"])\n",
        "        self.ffn_norm = RMSNorm(config[\"emb_dim\"])\n",
        "        self.attn = MultiHeadAttention(config)\n",
        "        self.ffn = FeedForward(config)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.attn_norm(x))\n",
        "        x = x + self.ffn(self.ffn_norm(x))\n",
        "        return x\n",
        "\n",
        "# =========================\n",
        "# 6Ô∏è‚É£ Llama32Model\n",
        "# =========================\n",
        "class Llama32Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(config[\"n_layers\"])])\n",
        "        self.norm = RMSNorm(config[\"emb_dim\"])\n",
        "        self.lm_head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
        "    def forward(self, input_ids):\n",
        "        x = self.embed(input_ids)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.norm(x)\n",
        "        return self.lm_head(x)"
      ],
      "metadata": {
        "id": "FAoekPrM_trJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Config ---\n",
        "LLAMA32_CONFIG = {\n",
        "    \"vocab_size\": 128_256,\n",
        "    \"context_length\": 131_072,\n",
        "    \"emb_dim\": 2048,\n",
        "    \"n_heads\": 32,\n",
        "    \"n_layers\": 16,\n",
        "    \"hidden_dim\": 8192,\n",
        "    \"n_kv_groups\": 8,\n",
        "    \"rope_base\": 500_000.0,\n",
        "    \"dtype\": torch.bfloat16,\n",
        "    \"rope_freq\": {\n",
        "        \"factor\": 32.0,\n",
        "        \"low_freq_factor\": 1.0,\n",
        "        \"high_freq_factor\": 4.0,\n",
        "        \"original_context_length\": 8192,\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "dL8qwQfgBdTv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dummy input test"
      ],
      "metadata": {
        "id": "nXEefQ1uDmuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = Llama32Model(LLAMA32_CONFIG).to(device).to(torch.bfloat16)\n",
        "\n",
        "dummy_input = torch.randint(\n",
        "    0,\n",
        "    LLAMA32_CONFIG[\"vocab_size\"],\n",
        "    (2, 128),\n",
        "    device=device\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(dummy_input)\n",
        "\n",
        "print(\"Output shape:\", logits.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQNxJSGwAI79",
        "outputId": "5448bfdb-94f8-4c89-9cae-53fe6e44493d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 128, 128256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer from hugging face"
      ],
      "metadata": {
        "id": "JiQk7wiNLoEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"  # or Llama-3.2-1B-Instruct\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    use_auth_token=\"\"\n",
        ")\n",
        "\n",
        "print(\"Tokenizer loaded!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-2ZiThfLDKO",
        "outputId": "7da91a1e-0618-463e-81bc-1f3918597e34"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## encode a sample sentence into tokens and Decode back to text"
      ],
      "metadata": {
        "id": "M0xx5LHTL_0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello world! This is a test of the LLaMA 3.2 tokenizer.\"\n",
        "\n",
        "encoded = tokenizer(\n",
        "    text,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(\"Token IDs:\", encoded[\"input_ids\"])\n",
        "\n",
        "\n",
        "decoded = tokenizer.decode(\n",
        "    encoded[\"input_ids\"][0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(\"Decoded text:\", decoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy2MJkwBLLPh",
        "outputId": "52238cb2-ec4e-4c66-db0e-8df73cf9ab4e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs: tensor([[128000,   9906,   1917,      0,   1115,    374,    264,   1296,    315,\n",
            "            279,    445,   8921,   4940,    220,     18,     13,     17,  47058,\n",
            "             13]])\n",
            "Decoded text: Hello world! This is a test of the LLaMA 3.2 tokenizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tokenizer test for edge cases"
      ],
      "metadata": {
        "id": "vFRrsJpqPVV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "TOKEN = \"\"\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    token=TOKEN\n",
        ")\n",
        "\n",
        "# üîë Explicitly set PAD = EOS (both token and id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# ---- Edge case batch test ----\n",
        "texts = [\"\", \" \", \"Hello\", \"üöÄ\"]\n",
        "\n",
        "encoded_batch = tokenizer(\n",
        "    texts,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "decoded_batch = [\n",
        "    tokenizer.decode(ids, skip_special_tokens=True)\n",
        "    for ids in encoded_batch[\"input_ids\"]\n",
        "]\n",
        "\n",
        "print(\"pad_token:\", tokenizer.pad_token)\n",
        "print(\"pad_token_id:\", tokenizer.pad_token_id)\n",
        "print(\"input_ids:\\n\", encoded_batch[\"input_ids\"])\n",
        "print(\"decoded:\", decoded_batch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vy4gvl8N42D",
        "outputId": "df8ed240-8ce0-4d60-a9e0-c4a081afb3ef"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pad_token: <|end_of_text|>\n",
            "pad_token_id: 128001\n",
            "input_ids:\n",
            " tensor([[128000, 128001, 128001, 128001],\n",
            "        [128000,    220, 128001, 128001],\n",
            "        [128000,   9906, 128001, 128001],\n",
            "        [128000,   9468,    248,    222]])\n",
            "decoded: ['', ' ', 'Hello', 'üöÄ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.clear()\n"
      ],
      "metadata": {
        "id": "VP9D91B0V0BO"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}