{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP7aRGPABzypq04aJroBJFN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"VTHNcQ6B6M6C","executionInfo":{"status":"ok","timestamp":1770031276621,"user_tz":0,"elapsed":3146,"user":{"displayName":"Ayeni Oluwatosin Olawale","userId":"15779955476633171344"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math"]},{"cell_type":"markdown","source":["# RMSNorm (Llama uses RMSNorm)"],"metadata":{"id":"QWgH-8uj97uX"}},{"cell_type":"code","source":["# --- RMSNorm ---\n","class RMSNorm(nn.Module):\n","    def __init__(self, dim, eps=1e-6):\n","        super().__init__()\n","        self.eps = eps\n","        self.weight = nn.Parameter(torch.ones(dim))\n","    def forward(self, x):\n","        norm = x.pow(2).mean(-1, keepdim=True)\n","        return x * torch.rsqrt(norm + self.eps) * self.weight\n"],"metadata":{"id":"cR5u0RcP9vVv","executionInfo":{"status":"ok","timestamp":1770031276636,"user_tz":0,"elapsed":2,"user":{"displayName":"Ayeni Oluwatosin Olawale","userId":"15779955476633171344"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Rotary Positional Embeddings (RoPE)"],"metadata":{"id":"tS26KTHS-A8S"}},{"cell_type":"code","source":["# --- RoPE ---\n","class RotaryEmbedding(nn.Module):\n","    def __init__(self, dim, base=500_000):\n","        super().__init__()\n","        self.dim = dim\n","        self.base = base\n","        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n","        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n","    def forward(self, seq_len, device, dtype):\n","        t = torch.arange(seq_len, device=device, dtype=dtype)\n","        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq.to(dtype))\n","        return torch.cos(freqs), torch.sin(freqs)\n","\n","def apply_rope(x, cos, sin):\n","    cos = cos.to(x.dtype)\n","    sin = sin.to(x.dtype)\n","    x1, x2 = x[..., ::2], x[..., 1::2]\n","    return torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n"],"metadata":{"id":"uoHw6fdf-M3g","executionInfo":{"status":"ok","timestamp":1770031276667,"user_tz":0,"elapsed":30,"user":{"displayName":"Ayeni Oluwatosin Olawale","userId":"15779955476633171344"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Llama Feed-Forward Network (SwiGLU)"],"metadata":{"id":"di8nBN4KCptJ"}},{"cell_type":"code","source":["# --- FeedForward ---\n","class FeedForward(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.gate = nn.Linear(config[\"emb_dim\"], config[\"hidden_dim\"], bias=False)\n","        self.up = nn.Linear(config[\"emb_dim\"], config[\"hidden_dim\"], bias=False)\n","        self.down = nn.Linear(config[\"hidden_dim\"], config[\"emb_dim\"], bias=False)\n","    def forward(self, x):\n","        return self.down(F.silu(self.gate(x)) * self.up(x))"],"metadata":{"id":"yH37m3WICn_e","executionInfo":{"status":"ok","timestamp":1770031276679,"user_tz":0,"elapsed":10,"user":{"displayName":"Ayeni Oluwatosin Olawale","userId":"15779955476633171344"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Grouped-Query Multi-Head Attention (GQA)"],"metadata":{"id":"EtmYeFc2-UnZ"}},{"cell_type":"code","source":["# --- MultiHeadAttention ---\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.emb_dim = config[\"emb_dim\"]\n","        self.n_heads = config[\"n_heads\"]\n","        self.n_kv = config[\"n_kv_groups\"]\n","        self.head_dim = self.emb_dim // self.n_heads\n","\n","        self.q_proj = nn.Linear(self.emb_dim, self.emb_dim, bias=False)\n","        self.k_proj = nn.Linear(self.emb_dim, self.n_kv * self.head_dim, bias=False)\n","        self.v_proj = nn.Linear(self.emb_dim, self.n_kv * self.head_dim, bias=False)\n","        self.o_proj = nn.Linear(self.emb_dim, self.emb_dim, bias=False)\n","\n","        self.rope = RotaryEmbedding(self.head_dim, base=config[\"rope_base\"])\n","\n","    def forward(self, x):\n","        B, T, C = x.shape\n","        dtype = x.dtype\n","\n","        # Project QKV\n","        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim)\n","        k = self.k_proj(x).view(B, T, self.n_kv, self.head_dim)\n","        v = self.v_proj(x).view(B, T, self.n_kv, self.head_dim)\n","\n","        # RoPE\n","        cos, sin = self.rope(T, x.device, dtype)\n","        cos, sin = cos[None, :, None, :], sin[None, :, None, :]\n","        q = apply_rope(q, cos, sin)\n","        k = apply_rope(k, cos, sin)\n","\n","        # Expand KV for grouped query\n","        k = k.repeat_interleave(self.n_heads // self.n_kv, dim=2)\n","        v = v.repeat_interleave(self.n_heads // self.n_kv, dim=2)\n","\n","        # Transpose for attention\n","        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n","\n","        # Attention scores\n","        attn_scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n","\n","        # Causal mask\n","        causal_mask = torch.tril(torch.ones(T, T, device=x.device, dtype=torch.bool))\n","        attn_scores = attn_scores.masked_fill(~causal_mask, float(\"-inf\"))\n","\n","        # Softmax & output\n","        attn_probs = F.softmax(attn_scores, dim=-1)\n","        out = attn_probs @ v\n","        out = out.transpose(1,2).contiguous().view(B, T, C)\n","        return self.o_proj(out)"],"metadata":{"id":"oUfTaxNw-dex","executionInfo":{"status":"ok","timestamp":1770031276707,"user_tz":0,"elapsed":23,"user":{"displayName":"Ayeni Oluwatosin Olawale","userId":"15779955476633171344"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Transfomer Blocks → Llama 3.2 Model"],"metadata":{"id":"tqfQ-DFh_yf1"}},{"cell_type":"code","source":["# --- Transformer Block ---\n","class TransformerBlock(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.attn_norm = RMSNorm(config[\"emb_dim\"])\n","        self.ffn_norm = RMSNorm(config[\"emb_dim\"])\n","        self.attn = MultiHeadAttention(config)\n","        self.ffn = FeedForward(config)\n","    def forward(self, x):\n","        x = x + self.attn(self.attn_norm(x))\n","        x = x + self.ffn(self.ffn_norm(x))\n","        return x\n","\n","# =========================\n","# 6️⃣ Llama32Model\n","# =========================\n","class Llama32Model(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.embed = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n","        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(config[\"n_layers\"])])\n","        self.norm = RMSNorm(config[\"emb_dim\"])\n","        self.lm_head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n","    def forward(self, input_ids):\n","        x = self.embed(input_ids)\n","        for block in self.blocks:\n","            x = block(x)\n","        x = self.norm(x)\n","        return self.lm_head(x)"],"metadata":{"id":"FAoekPrM_trJ","executionInfo":{"status":"ok","timestamp":1770031276766,"user_tz":0,"elapsed":43,"user":{"displayName":"Ayeni Oluwatosin Olawale","userId":"15779955476633171344"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# --- Config ---\n","LLAMA32_CONFIG = {\n","    \"vocab_size\": 128_256,\n","    \"context_length\": 131_072,\n","    \"emb_dim\": 2048,\n","    \"n_heads\": 32,\n","    \"n_layers\": 16,\n","    \"hidden_dim\": 8192,\n","    \"n_kv_groups\": 8,\n","    \"rope_base\": 500_000.0,\n","    \"dtype\": torch.bfloat16,\n","    \"rope_freq\": {\n","        \"factor\": 32.0,\n","        \"low_freq_factor\": 1.0,\n","        \"high_freq_factor\": 4.0,\n","        \"original_context_length\": 8192,\n","    }\n","}\n"],"metadata":{"id":"dL8qwQfgBdTv","executionInfo":{"status":"ok","timestamp":1770031276769,"user_tz":0,"elapsed":2,"user":{"displayName":"Ayeni Oluwatosin Olawale","userId":"15779955476633171344"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## dummy input test"],"metadata":{"id":"nXEefQ1uDmuz"}},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = Llama32Model(LLAMA32_CONFIG).to(device).to(torch.bfloat16)\n","\n","dummy_input = torch.randint(\n","    0,\n","    LLAMA32_CONFIG[\"vocab_size\"],\n","    (2, 128),\n","    device=device\n",")\n","\n","with torch.no_grad():\n","    logits = model(dummy_input)\n","\n","print(\"Output shape:\", logits.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KQNxJSGwAI79","executionInfo":{"status":"ok","timestamp":1770031357958,"user_tz":0,"elapsed":81189,"user":{"displayName":"Ayeni Oluwatosin Olawale","userId":"15779955476633171344"}},"outputId":"ef7808ea-463b-4f06-9517-dd875039146c"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Output shape: torch.Size([2, 128, 128256])\n"]}]}]}