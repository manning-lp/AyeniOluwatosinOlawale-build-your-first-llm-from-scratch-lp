{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNqbb+x/5C3FrrR5PQqaJtO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","\n","# LLAMA32 config\n","LLAMA32_CONFIG = {\n","    \"vocab_size\": 128_256,\n","    \"context_length\": 131_072,\n","    \"emb_dim\": 2048,\n","    \"n_heads\": 32,\n","    \"n_layers\": 16,\n","    \"hidden_dim\": 8192,\n","    \"n_kv_groups\": 8,\n","    \"rope_base\": 500_000.0,\n","    \"dtype\": torch.bfloat16,\n","    \"rope_freq\": {\n","        \"factor\": 32.0,\n","        \"low_freq_factor\": 1.0,\n","        \"high_freq_factor\": 4.0,\n","        \"original_context_length\": 8192,\n","    }\n","}\n","\n","def apply_rope(x, base=500_000.0):\n","    batch, seq_len, dim = x.shape\n","    half_dim = dim // 2\n","    freq_seq = torch.arange(half_dim, dtype=x.dtype, device=x.device)\n","    freq_seq = 1.0 / (base ** (freq_seq / half_dim))\n","    positions = torch.arange(seq_len, dtype=x.dtype, device=x.device)\n","    angles = torch.einsum(\"i,j->ij\", positions, freq_seq)\n","    cos = torch.cos(angles)\n","    sin = torch.sin(angles)\n","    x1, x2 = x[..., :half_dim], x[..., half_dim:]\n","    return torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.emb_dim = config[\"emb_dim\"]\n","        self.n_heads = config[\"n_heads\"]\n","        self.head_dim = self.emb_dim // self.n_heads\n","        self.qkv_proj = nn.Linear(self.emb_dim, self.emb_dim * 3)\n","        self.out_proj = nn.Linear(self.emb_dim, self.emb_dim)\n","\n","    def forward(self, x, kv_cache=None):\n","        batch, seq_len, emb_dim = x.shape\n","        x = apply_rope(x, base=LLAMA32_CONFIG[\"rope_base\"])\n","        qkv = self.qkv_proj(x)\n","        qkv = qkv.reshape(batch, seq_len, 3, self.n_heads, self.head_dim)\n","        q, k, v = qkv[:, :, 0], qkv[:, :, 1], qkv[:, :, 2]\n","        q = q.transpose(1, 2)\n","        k = k.transpose(1, 2)\n","        v = v.transpose(1, 2)\n","\n","        # Append to cache if given\n","        if kv_cache is not None:\n","            k = torch.cat([kv_cache[\"k\"], k], dim=2)\n","            v = torch.cat([kv_cache[\"v\"], v], dim=2)\n","            kv_cache[\"k\"], kv_cache[\"v\"] = k, v\n","        else:\n","            kv_cache = {\"k\": k, \"v\": v}\n","\n","        # Efficient causal mask\n","        seq_len_total = k.size(2)\n","        mask = torch.tril(torch.ones(seq_len_total, seq_len_total, device=x.device)).unsqueeze(0).unsqueeze(0)\n","        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n","        attn_scores = attn_scores.masked_fill(mask[:, :, -seq_len:, :] == 0, float('-inf'))\n","        attn_probs = F.softmax(attn_scores, dim=-1)\n","        out = torch.matmul(attn_probs, v)\n","        out = out.transpose(1, 2).reshape(batch, seq_len, emb_dim)\n","        out = self.out_proj(out)\n","        return out, kv_cache\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.fc1 = nn.Linear(config[\"emb_dim\"], config[\"hidden_dim\"])\n","        self.activation = nn.GELU()\n","        self.fc2 = nn.Linear(config[\"hidden_dim\"], config[\"emb_dim\"])\n","\n","    def forward(self, x):\n","        return self.fc2(self.activation(self.fc1(x)))\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.attn = MultiHeadAttention(config)\n","        self.ffn = FeedForward(config)\n","        self.ln1 = nn.LayerNorm(config[\"emb_dim\"], eps=1e-5)\n","        self.ln2 = nn.LayerNorm(config[\"emb_dim\"], eps=1e-5)\n","\n","    def forward(self, x, kv_cache=None):\n","        attn_out, kv_cache = self.attn(self.ln1(x), kv_cache)\n","        x = x + attn_out\n","        x = x + self.ffn(self.ln2(x))\n","        return x, kv_cache\n","\n","class LLAMA32(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        self.token_emb = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n","        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(config[\"n_layers\"])])\n","        self.ln_final = nn.LayerNorm(config[\"emb_dim\"], eps=1e-5)\n","        self.head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n","\n","    def forward(self, input_ids, kv_caches=None):\n","        x = self.token_emb(input_ids)\n","        new_caches = []\n","        for i, block in enumerate(self.blocks):\n","            kv_cache = None if kv_caches is None else kv_caches[i]\n","            x, cache = block(x, kv_cache)\n","            new_caches.append(cache)\n","        x = self.ln_final(x)\n","        logits = self.head(x)\n","        return logits, new_caches\n","\n","# Testing with long sequences\n","batch_size = 1\n","seq_length = 1024  # shorter than max for testing\n","dummy_input_ids = torch.randint(0, LLAMA32_CONFIG[\"vocab_size\"], (batch_size, seq_length))\n","model = LLAMA32(LLAMA32_CONFIG).to(dtype=torch.bfloat16)\n","\n","logits, caches = model(dummy_input_ids)\n","print(\"Logits shape:\", logits.shape)  # [1, 1024, vocab_size]\n","print(\"KV cache for first block k shape:\", caches[0][\"k\"].shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qoyOYeM-3gu_","executionInfo":{"status":"ok","timestamp":1768854558537,"user_tz":0,"elapsed":105924,"user":{"displayName":"Ayeni Oluwatosin Olawale","userId":"15779955476633171344"}},"outputId":"77218205-a840-4881-c07a-15dbca981027"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Logits shape: torch.Size([1, 1024, 128256])\n","KV cache for first block k shape: torch.Size([1, 32, 1024, 64])\n"]}]}]}