{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VTHNcQ6B6M6C"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SXy2XPMNibtv"
   },
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "LLAMA32_CONFIG = {\n",
    "    \"vocab_size\": 128_256,\n",
    "    \"context_length\": 4096,\n",
    "    \"emb_dim\": 2048,\n",
    "    \"n_heads\": 32,\n",
    "    \"n_layers\": 16,\n",
    "    \"hidden_dim\": 8192,\n",
    "    \"n_kv_groups\": 8,\n",
    "    \"rope_base\": 500_000.0,\n",
    "    \"dtype\": torch.bfloat16,\n",
    "    \"rope_freq\": {\n",
    "        \"factor\": 32.0,\n",
    "        \"low_freq_factor\": 1.0,\n",
    "        \"high_freq_factor\": 4.0,\n",
    "        \"original_context_length\": 8192,\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWgH-8uj97uX"
   },
   "source": [
    "# RMSNorm (Llama uses RMSNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cR5u0RcP9vVv"
   },
   "outputs": [],
   "source": [
    "# --- RMSNorm ---\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    def forward(self, x):\n",
    "        norm = x.pow(2).mean(-1, keepdim=True)\n",
    "        return x * torch.rsqrt(norm + self.eps) * self.weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tS26KTHS-A8S"
   },
   "source": [
    "## Rotary Positional Embeddings (RoPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uoHw6fdf-M3g"
   },
   "outputs": [],
   "source": [
    "# --- RoPE ---\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, base=500_000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "    def forward(self, seq_len, device, dtype):\n",
    "        t = torch.arange(seq_len, device=device, dtype=dtype)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq.to(dtype))\n",
    "        return torch.cos(freqs), torch.sin(freqs)\n",
    "\n",
    "def apply_rope(x, cos, sin):\n",
    "    cos = cos.to(x.dtype)\n",
    "    sin = sin.to(x.dtype)\n",
    "    x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "    return torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "di8nBN4KCptJ"
   },
   "source": [
    "# Llama Feed-Forward Network (SwiGLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yH37m3WICn_e"
   },
   "outputs": [],
   "source": [
    "# --- FeedForward ---\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(config[\"emb_dim\"], config[\"hidden_dim\"], bias=False)\n",
    "        self.up = nn.Linear(config[\"emb_dim\"], config[\"hidden_dim\"], bias=False)\n",
    "        self.down = nn.Linear(config[\"hidden_dim\"], config[\"emb_dim\"], bias=False)\n",
    "    def forward(self, x):\n",
    "        return self.down(F.silu(self.gate(x)) * self.up(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtmYeFc2-UnZ"
   },
   "source": [
    "# Grouped-Query Multi-Head Attention (GQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oUfTaxNw-dex"
   },
   "outputs": [],
   "source": [
    "# --- MultiHeadAttention ---\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.emb_dim = config[\"emb_dim\"]\n",
    "        self.n_heads = config[\"n_heads\"]\n",
    "        self.n_kv = config[\"n_kv_groups\"]\n",
    "        self.head_dim = self.emb_dim // self.n_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(self.emb_dim, self.emb_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.emb_dim, self.n_kv * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.emb_dim, self.n_kv * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.emb_dim, self.emb_dim, bias=False)\n",
    "\n",
    "        self.rope = RotaryEmbedding(self.head_dim, base=config[\"rope_base\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        dtype = x.dtype\n",
    "\n",
    "        # Project QKV\n",
    "        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim)\n",
    "        k = self.k_proj(x).view(B, T, self.n_kv, self.head_dim)\n",
    "        v = self.v_proj(x).view(B, T, self.n_kv, self.head_dim)\n",
    "\n",
    "        # RoPE\n",
    "        cos, sin = self.rope(T, x.device, dtype)\n",
    "        cos, sin = cos[None, :, None, :], sin[None, :, None, :]\n",
    "        q = apply_rope(q, cos, sin)\n",
    "        k = apply_rope(k, cos, sin)\n",
    "\n",
    "        # Expand KV for grouped query\n",
    "        k = k.repeat_interleave(self.n_heads // self.n_kv, dim=2)\n",
    "        v = v.repeat_interleave(self.n_heads // self.n_kv, dim=2)\n",
    "\n",
    "        # Transpose for attention\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        # Attention scores\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # Causal mask\n",
    "        causal_mask = torch.tril(torch.ones(T, T, device=x.device, dtype=torch.bool))\n",
    "        attn_scores = attn_scores.masked_fill(~causal_mask, float(\"-inf\"))\n",
    "\n",
    "        # Softmax & output\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        out = attn_probs @ v\n",
    "        out = out.transpose(1,2).contiguous().view(B, T, C)\n",
    "        return self.o_proj(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqfQ-DFh_yf1"
   },
   "source": [
    "## Transfomer Blocks ‚Üí Llama 3.2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FAoekPrM_trJ"
   },
   "outputs": [],
   "source": [
    "# --- Transformer Block ---\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn_norm = RMSNorm(config[\"emb_dim\"])\n",
    "        self.ffn_norm = RMSNorm(config[\"emb_dim\"])\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.ffn = FeedForward(config)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.attn_norm(x))\n",
    "        x = x + self.ffn(self.ffn_norm(x))\n",
    "        return x\n",
    "\n",
    "# =========================\n",
    "# 6Ô∏è‚É£ Llama32Model\n",
    "# =========================\n",
    "class Llama32Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(config[\"n_layers\"])])\n",
    "        self.norm = RMSNorm(config[\"emb_dim\"])\n",
    "        self.lm_head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embed(input_ids)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "        return self.lm_head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGq3NZw2kW1y"
   },
   "source": [
    "# Test dummy input + Load the model on hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "voPnu6mTl8Mh",
    "outputId": "ed182c52-faf8-477c-eec3-2bb508f35201"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "juk85RxFkXh8",
    "outputId": "7f2527db-c0da-4e6f-8d73-0bacf8f2faad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 128, 128256])\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Llama32Model(LLAMA32_CONFIG).to(device).to(torch.bfloat16)\n",
    "dummy_input = torch.randint(\n",
    "    0,\n",
    "    LLAMA32_CONFIG[\"vocab_size\"],\n",
    "    (2, 128),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(dummy_input)\n",
    "\n",
    "print(\"Output shape:\", logits.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Q2sxifokb9u"
   },
   "source": [
    "# Tokenizer from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bdr1iLxgSLoU",
    "outputId": "6910751f-7f84-4a5b-9dac-d594623b3918"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!pip install -q huggingface_hub transformers accelerate\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xyS9g06kzPLb",
    "outputId": "0380b129-07ef-4866-9c02-89ff3bc63e9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Tokenizer loaded!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYYgb4raknOd"
   },
   "source": [
    "# encode a sample sentence into tokens and Decode back to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fce0K2TBknt1",
    "outputId": "4046645a-00c6-4e52-8563-2c35294627ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([[128000,   9906,   1917,      0,   1115,    374,    264,   1296,    315,\n",
      "            279,    445,   8921,   4940,    220,     18,     13,     17,  47058,\n",
      "             13]])\n",
      "Decoded text: Hello world! This is a test of the LLaMA 3.2 tokenizer.\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello world! This is a test of the LLaMA 3.2 tokenizer.\"\n",
    "\n",
    "encoded = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(\"Token IDs:\", encoded[\"input_ids\"])\n",
    "\n",
    "\n",
    "decoded = tokenizer.decode(\n",
    "    encoded[\"input_ids\"][0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(\"Decoded text:\", decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhwJfhJZk1Rf"
   },
   "source": [
    "## tokenizer test for edge cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3te129ukvAo",
    "outputId": "67e9c5c4-90fe-4bab-a49c-3e7b273df46d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token: <|end_of_text|>\n",
      "pad_token_id: 128001\n",
      "input_ids:\n",
      " tensor([[128000, 128001, 128001, 128001],\n",
      "        [128000,    220, 128001, 128001],\n",
      "        [128000,   9906, 128001, 128001],\n",
      "        [128000,   9468,    248,    222]])\n",
      "decoded: ['', ' ', 'Hello', 'üöÄ']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKEN = \"\"\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=TOKEN\n",
    ")\n",
    "\n",
    "# üîë Explicitly set PAD = EOS (both token and id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# ---- Edge case batch test ----\n",
    "texts = [\"\", \" \", \"Hello\", \"üöÄ\"]\n",
    "\n",
    "encoded_batch = tokenizer(\n",
    "    texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "decoded_batch = [\n",
    "    tokenizer.decode(ids, skip_special_tokens=True)\n",
    "    for ids in encoded_batch[\"input_ids\"]\n",
    "]\n",
    "\n",
    "print(\"pad_token:\", tokenizer.pad_token)\n",
    "print(\"pad_token_id:\", tokenizer.pad_token_id)\n",
    "print(\"input_ids:\\n\", encoded_batch[\"input_ids\"])\n",
    "print(\"decoded:\", decoded_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCOCngaPnUwd"
   },
   "source": [
    "### Access to the Llama 3.2 weights + simple test model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "oDneKdgsmcdf"
   },
   "outputs": [],
   "source": [
    "!pip install transformers accelerate --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "85896ede5dd244e78a0cbda20ae3a08a",
      "0ecf43fdf4ad4d75b5dc0d3d2018066f",
      "22312b554a0049808ec2f02235e2a932",
      "7dc0bbcb6eae46a48f84d835e03e952d",
      "2e4301a2229f4043a85db1fee998d0c1",
      "f4ad1c8cf1744e58a42865312c8e9619",
      "2238581518d04c5ca18a21541b529d96",
      "a2f42d2be86742c8bb477a4b946497bf",
      "2a63d7096f6142eca37af56c23b95c68",
      "cdaa43eab25c421182a7d0a961c6b38e",
      "5f69ab425ec341a3828c883e17dfa82f"
     ]
    },
    "id": "rkP1-kVH0DjD",
    "outputId": "8d04fa70-4055-42c1-95a9-112217db0390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "HF_TOKEN = \"\"   # paste new token here\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=HF_TOKEN,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"   # recommended\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(\"Model and tokenizer loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lJCIEycqfWT"
   },
   "source": [
    "## Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "d2D3BAYvp-44"
   },
   "outputs": [],
   "source": [
    "input_text = \"Once upon a time,\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0R7z-ozqBaA",
    "outputId": "4f29a714-b0ee-49f3-815b-2c140930e10b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token prediction:  in\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "    next_token_logits = logits[:, -1, :]  # last token\n",
    "    next_token_id = torch.argmax(next_token_logits, dim=-1)\n",
    "\n",
    "predicted_token = tokenizer.decode(next_token_id)\n",
    "print(\"Next token prediction:\", predicted_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "D3nS93KKqSow"
   },
   "outputs": [],
   "source": [
    "def predict_next_tokens(prompt, max_new_tokens=10):\n",
    "    \"\"\"\n",
    "    Generate tokens iteratively using greedy decoding.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): Input text.\n",
    "        max_new_tokens (int): Number of tokens to predict.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated text (prompt + new tokens)\n",
    "    \"\"\"\n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids).logits  # [batch, seq_len, vocab_size]\n",
    "            next_token_logits = logits[:, -1, :]  # last token logits\n",
    "            next_token_id = torch.argmax(next_token_logits, dim=-1)  # greedy\n",
    "        # Append predicted token\n",
    "        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "    # Decode the entire sequence\n",
    "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TboO12dXqVDc",
    "outputId": "d729e12d-5959-4376-e504-779b02948349"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      " Once upon a time, in a small village nestled in the rolling hills of the countryside, there lived a young girl named Sophia\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time,\"\n",
    "generated = predict_next_tokens(prompt, max_new_tokens=20)\n",
    "print(\"Generated text:\\n\", generated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8A-QQ1hdrpCX"
   },
   "source": [
    "## Prepare a small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "wZs7RaQprQTS"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class SmallDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "        for t in texts:\n",
    "            enc = tokenizer(t, truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            self.examples.append(torch.tensor(enc[\"input_ids\"]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "# Sample dataset\n",
    "texts = [\n",
    "    \"Once upon a time, there was a brave knight.\",\n",
    "    \"The stock market showed significant gains today.\",\n",
    "    \"Machine learning can predict trends in healthcare.\",\n",
    "    \"Artificial intelligence is transforming science.\"\n",
    "]\n",
    "\n",
    "dataset = SmallDataset(texts, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "jXAZODxks1KJ"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.optim import AdamW   # <-- correct\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "4da6c9a4f4e84ebf98fbc6d47a0fbdf4",
      "2d166629249d4ff4aff7928af6237be9",
      "2bdca183e93b44679140f45435b16792",
      "f8e3d23b474d472bb4d2061c41fe54d6",
      "c567c128942c486a86b243394ac109e9",
      "e130793ef28445c7956da1ae13f1aff1",
      "6200a1fca7cc47aaaca3ecf93be446d9",
      "efa591dec99d4e73890ab512f97bad32",
      "84652c9593e344fab96371482e20caa3",
      "a51635024db244b09e77c81d1b74a2cc",
      "9b02ed5157aa408c9b5e365eba5a4db8"
     ]
    },
    "id": "OiRgpgV80WEh",
    "outputId": "477a29e3-5aa0-4b99-c5a1-43c1b71bd6fd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.optim import AdamW\n",
    "\n",
    "HF_TOKEN = \"\"\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=HF_TOKEN,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"   # VERY important for 3B\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eXsk-EW_tI5a",
    "outputId": "53fbbaa3-0c37-427f-9977-2449b8504ebb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 3.5987\n",
      "Epoch 2/20 - Loss: 0.2796\n",
      "Epoch 3/20 - Loss: 0.1406\n",
      "Epoch 4/20 - Loss: 0.1144\n",
      "Epoch 5/20 - Loss: 0.0905\n",
      "Epoch 6/20 - Loss: 0.0785\n",
      "Epoch 7/20 - Loss: 0.0681\n",
      "Epoch 8/20 - Loss: 0.0577\n",
      "Epoch 9/20 - Loss: 0.0475\n",
      "Epoch 10/20 - Loss: 0.0368\n",
      "Epoch 11/20 - Loss: 0.0283\n",
      "Epoch 12/20 - Loss: 0.0242\n",
      "Epoch 13/20 - Loss: 0.0264\n",
      "Epoch 14/20 - Loss: 0.0257\n",
      "Epoch 15/20 - Loss: 0.0261\n",
      "Epoch 16/20 - Loss: 0.0260\n",
      "Epoch 17/20 - Loss: 0.0261\n",
      "Epoch 18/20 - Loss: 0.0254\n",
      "Epoch 19/20 - Loss: 0.0247\n",
      "Epoch 20/20 - Loss: 0.0237\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(batch, labels=batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIsT1JXpt4OL"
   },
   "source": [
    "## Weights downloaded and loaded into model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ACvu0JuFbbqG"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "HF_TOKEN = \"\"\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99,
     "referenced_widgets": [
      "01459413dc4f43179b12a2ab4960c3a8",
      "45feb60de1454f8eb72452f557adf2aa",
      "b9030db2a21349f29ab5bf30ab9dd70a",
      "02048d22b6ee4ffd8c56c77e16e009cd",
      "1cd53ebb1aa444eb9d0380b536606351",
      "4877010f5caa44598359b1b9e1f43aaa",
      "ee482d6b593941fca727dbb0a9222a43",
      "ee1cdbff4e3a4fa0ae538f70236d9fa3",
      "aab215e1ca0a4282955572f83de2ef4c",
      "6a4c235fa2194c9b81161fae0382ca6a",
      "5b6375a3ed5b429581992bc26b9df252",
      "5b0231b5488c4f2b891484df1cb0bbab",
      "7f2c85db65654042a88d7ba8145de757",
      "0a3f398eabec46bcbee77ab5b0022a28",
      "4b025675da374757bb3e476a9794f284",
      "10d531e0f92048cb97c480755abc76fb",
      "e44dbc308cfc454593de862160344118",
      "a8e14dfffcd346b6b4cecb3d7db4e10c",
      "a46cd5e6a16649dfbd8afbc25359a3ec",
      "4a543edbdbae4d4ebfee379c0b4552c0",
      "bb5a1b04a60b469387bba474eb6f0ac7",
      "65ca3c2ebbe345298558184d8056fdd1"
     ]
    },
    "id": "a8yl0stabeA5",
    "outputId": "2d093193-5e58-4b0c-c1c8-01a5251f4132"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/llama32_weights'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "repo_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "local_dir = \"./llama32_weights\"\n",
    "\n",
    "snapshot_download(repo_id, local_dir=local_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500,
     "referenced_widgets": [
      "23b4beb3a63f48f8a3c103b16075a29d",
      "90f154e334e44da18ae52620cbf1e3cc",
      "d919c20ce26a42219d38f0056694ac72",
      "2d886cffcbbd4117a45477b9724e7f5a",
      "be087e8096a24729a66350ccc88944d1",
      "a94d9e4785b3433c950fb0d2af1b0c6f",
      "a841ef16fc194df3bfa2ef201982b2f2",
      "0bdf93cb0f68440db2c211f2b76d1234",
      "a94b9aa3f14e400dad2d2abd0fef428b",
      "f202975ac54c4e2484dfd5dd341a1c77",
      "f7af3c40c87844b19fd90295e2d84438"
     ]
    },
    "id": "UJ6Cf5ifvJhq",
    "outputId": "0ed3d200-9989-4716-eda2-4436526e357e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer locally\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model locally\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_dir,\n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CclTykubvN8v",
    "outputId": "8215823c-192c-459e-be3b-ea7511c2f49f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token:  in\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time,\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    next_token_id = torch.argmax(outputs.logits[:, -1, :], dim=-1)\n",
    "\n",
    "print(\"Next token:\", tokenizer.decode(next_token_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "P_FUtLqBvbnR"
   },
   "outputs": [],
   "source": [
    "def predict_next_tokens(prompt, max_new_tokens=10):\n",
    "    \"\"\"\n",
    "    Generate tokens iteratively using greedy decoding.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): Input text.\n",
    "        max_new_tokens (int): Number of tokens to predict.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated text (prompt + new tokens)\n",
    "    \"\"\"\n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids).logits  # [batch, seq_len, vocab_size]\n",
    "            next_token_logits = logits[:, -1, :]  # last token logits\n",
    "            next_token_id = torch.argmax(next_token_logits, dim=-1)  # greedy\n",
    "        # Append predicted token\n",
    "        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "    # Decode the entire sequence\n",
    "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0LfPcCe0vh2w",
    "outputId": "09f891ed-7e24-4dd7-dd7f-5b47c31831c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      " Once upon a time, in a small village nestled in the rolling hills of the countryside, there lived a young girl named Sophia\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time,\"\n",
    "generated = predict_next_tokens(prompt, max_new_tokens=20)\n",
    "print(\"Generated text:\\n\", generated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsIZ-Zpkfz6T"
   },
   "source": [
    "## A simple prompt encode,generate tokens and decode to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SFiFHPEBdAwA",
    "outputId": "9a2396d7-c1b3-432b-f427-764bad8b1cb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Artificial intelligence is transforming\n",
      "Generated: Artificial intelligence is transforming the way we live and work. From smart homes to self-driving cars, AI is being used in various industries to improve efficiency, productivity, and customer experience. However, as AI becomes more prevalent, there are also concerns about its potential impact on jobs\n"
     ]
    }
   ],
   "source": [
    "# --- Simple prompt ---\n",
    "prompt = \"Artificial intelligence is transforming\"\n",
    "\n",
    "# --- Encode the prompt ---\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# --- Generate tokens ---\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,      # number of tokens to generate\n",
    "        do_sample=True,         # enable sampling for creativity\n",
    "        temperature=0.7,        # control randomness\n",
    "        pad_token_id=tokenizer.pad_token_id  # ensures padding token is used\n",
    "    )\n",
    "\n",
    "# --- Decode generated tokens ---\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"Generated:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQfUZtp2gZbY"
   },
   "source": [
    "## A clean_text function and produce polished, conversational outputs with chat format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ns7jOdghfLbC",
    "outputId": "b4771fc1-5026-4389-d62c-f06a44a0cced"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Prompt:\n",
      " User: Hello! Can you explain AI in simple terms?\n",
      "User: Also, give a quick example in healthcare.\n",
      "Assistant: \n",
      "\n",
      "Assistant Reply:\n",
      " **What is AI?** Artificial Intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence, such as: * Learning * Problem-solving * Reasoning * Perception **How does AI work?** AI systems use algorithms and data to make decisions, predictions, or take actions. These systems can be trained on vast amounts of data, allowing them to learn from experience and improve over time. **Example in Healthcare:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Chat formatting class ---\n",
    "class ChatFormat:\n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "\n",
    "    def add_message(self, role: str, content: str):\n",
    "        \"\"\"Add a message with a role: 'user' or 'assistant'\"\"\"\n",
    "        self.messages.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    def get_prompt(self):\n",
    "        \"\"\"Construct the full prompt text for the model\"\"\"\n",
    "        prompt_text = \"\"\n",
    "        for msg in self.messages:\n",
    "            prompt_text += f\"{msg['role'].capitalize()}: {msg['content']}\\n\"\n",
    "        prompt_text += \"Assistant: \"\n",
    "        return prompt_text\n",
    "\n",
    "# --- Clean text function ---\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove repeated EOS tokens, extra whitespaces, and make output readable\n",
    "    \"\"\"\n",
    "    # Remove tokenizer special tokens\n",
    "    text = text.replace(tokenizer.eos_token, \"\")\n",
    "    # Collapse multiple spaces/newlines\n",
    "    text = \" \".join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "# --- Example conversation ---\n",
    "chat = ChatFormat()\n",
    "chat.add_message(\"user\", \"Hello! Can you explain AI in simple terms?\")\n",
    "chat.add_message(\"user\", \"Also, give a quick example in healthcare.\")\n",
    "\n",
    "# --- Build prompt ---\n",
    "prompt = chat.get_prompt()\n",
    "\n",
    "# --- Tokenize and generate ---\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "# --- Decode and clean ---\n",
    "raw_output = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "assistant_reply = raw_output.split(\"Assistant:\")[-1]\n",
    "assistant_reply = clean_text(assistant_reply)\n",
    "\n",
    "# --- Show final polished response ---\n",
    "print(\"User Prompt:\\n\", prompt)\n",
    "print(\"\\nAssistant Reply:\\n\", assistant_reply)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "H100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
