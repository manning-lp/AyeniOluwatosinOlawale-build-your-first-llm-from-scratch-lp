{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a9229e296e174c2289bde58db5fcf7fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8037a3d902914e6496d0e386c4762449",
              "IPY_MODEL_0616adf0f9bb4649b39bdb712a935152",
              "IPY_MODEL_1f86cb20068e42b0b76f0148ce194ad9"
            ],
            "layout": "IPY_MODEL_553d525d1f6a493981411095c326beaf"
          }
        },
        "8037a3d902914e6496d0e386c4762449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11eba5ba0e0347f58d9532026fd73024",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3c2e9a7521744127b0039200c95a3753",
            "value": "Fetching‚Äá16‚Äáfiles:‚Äá100%"
          }
        },
        "0616adf0f9bb4649b39bdb712a935152": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_953e5e2714dc45699b26191c6d380592",
            "max": 16,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e5117941a313426f9a18ce91ade70ff4",
            "value": 16
          }
        },
        "1f86cb20068e42b0b76f0148ce194ad9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_775288aa028c49ef8e7b5a086e05d8d4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9eb86708fdd54bf89272a0f29b8a5f5a",
            "value": "‚Äá16/16‚Äá[00:00&lt;00:00,‚Äá833.22it/s]"
          }
        },
        "553d525d1f6a493981411095c326beaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11eba5ba0e0347f58d9532026fd73024": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c2e9a7521744127b0039200c95a3753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "953e5e2714dc45699b26191c6d380592": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5117941a313426f9a18ce91ade70ff4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "775288aa028c49ef8e7b5a086e05d8d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eb86708fdd54bf89272a0f29b8a5f5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7dc19afc174540b8ae6d36f4f7f736e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c3494e3330646e18fbfff48c7640645",
              "IPY_MODEL_7bf7055b8d96476b9dce8dbbd369cef5",
              "IPY_MODEL_ac3313c7a39940f19b11cbcf5246accd"
            ],
            "layout": "IPY_MODEL_2c3fae2aa48342dea333dae5769e44c0"
          }
        },
        "4c3494e3330646e18fbfff48c7640645": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d621d4abf6794eb08381c4fcdfcf9a63",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_394b3b3ac07a4423ad583a1076a1bda0",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "7bf7055b8d96476b9dce8dbbd369cef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f3965854c11497490958bd1192ee3f7",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_296ba4a3d4bf454d8557ef98ebeeff7f",
            "value": 2
          }
        },
        "ac3313c7a39940f19b11cbcf5246accd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ab0d196ba12488490cd19bdf6d9c946",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_399b3747d71641e98b923bbefd37dd58",
            "value": "‚Äá2/2‚Äá[00:00&lt;00:00,‚Äá46.42it/s]"
          }
        },
        "2c3fae2aa48342dea333dae5769e44c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d621d4abf6794eb08381c4fcdfcf9a63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "394b3b3ac07a4423ad583a1076a1bda0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f3965854c11497490958bd1192ee3f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "296ba4a3d4bf454d8557ef98ebeeff7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ab0d196ba12488490cd19bdf6d9c946": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "399b3747d71641e98b923bbefd37dd58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTHNcQ6B6M6C"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Config ---\n",
        "LLAMA32_CONFIG = {\n",
        "    \"vocab_size\": 128_256,\n",
        "    \"context_length\": 4096,\n",
        "    \"emb_dim\": 2048,\n",
        "    \"n_heads\": 32,\n",
        "    \"n_layers\": 16,\n",
        "    \"hidden_dim\": 8192,\n",
        "    \"n_kv_groups\": 8,\n",
        "    \"rope_base\": 500_000.0,\n",
        "    \"dtype\": torch.bfloat16,\n",
        "    \"rope_freq\": {\n",
        "        \"factor\": 32.0,\n",
        "        \"low_freq_factor\": 1.0,\n",
        "        \"high_freq_factor\": 4.0,\n",
        "        \"original_context_length\": 8192,\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "SXy2XPMNibtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RMSNorm (Llama uses RMSNorm)"
      ],
      "metadata": {
        "id": "QWgH-8uj97uX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- RMSNorm ---\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        norm = x.pow(2).mean(-1, keepdim=True)\n",
        "        return x * torch.rsqrt(norm + self.eps) * self.weight\n"
      ],
      "metadata": {
        "id": "cR5u0RcP9vVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rotary Positional Embeddings (RoPE)"
      ],
      "metadata": {
        "id": "tS26KTHS-A8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- RoPE ---\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim, base=500_000):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.base = base\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
        "    def forward(self, seq_len, device, dtype):\n",
        "        t = torch.arange(seq_len, device=device, dtype=dtype)\n",
        "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq.to(dtype))\n",
        "        return torch.cos(freqs), torch.sin(freqs)\n",
        "\n",
        "def apply_rope(x, cos, sin):\n",
        "    cos = cos.to(x.dtype)\n",
        "    sin = sin.to(x.dtype)\n",
        "    x1, x2 = x[..., ::2], x[..., 1::2]\n",
        "    return torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n"
      ],
      "metadata": {
        "id": "uoHw6fdf-M3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama Feed-Forward Network (SwiGLU)"
      ],
      "metadata": {
        "id": "di8nBN4KCptJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FeedForward ---\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.gate = nn.Linear(config[\"emb_dim\"], config[\"hidden_dim\"], bias=False)\n",
        "        self.up = nn.Linear(config[\"emb_dim\"], config[\"hidden_dim\"], bias=False)\n",
        "        self.down = nn.Linear(config[\"hidden_dim\"], config[\"emb_dim\"], bias=False)\n",
        "    def forward(self, x):\n",
        "        return self.down(F.silu(self.gate(x)) * self.up(x))"
      ],
      "metadata": {
        "id": "yH37m3WICn_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grouped-Query Multi-Head Attention (GQA)"
      ],
      "metadata": {
        "id": "EtmYeFc2-UnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MultiHeadAttention ---\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.emb_dim = config[\"emb_dim\"]\n",
        "        self.n_heads = config[\"n_heads\"]\n",
        "        self.n_kv = config[\"n_kv_groups\"]\n",
        "        self.head_dim = self.emb_dim // self.n_heads\n",
        "\n",
        "        self.q_proj = nn.Linear(self.emb_dim, self.emb_dim, bias=False)\n",
        "        self.k_proj = nn.Linear(self.emb_dim, self.n_kv * self.head_dim, bias=False)\n",
        "        self.v_proj = nn.Linear(self.emb_dim, self.n_kv * self.head_dim, bias=False)\n",
        "        self.o_proj = nn.Linear(self.emb_dim, self.emb_dim, bias=False)\n",
        "\n",
        "        self.rope = RotaryEmbedding(self.head_dim, base=config[\"rope_base\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        dtype = x.dtype\n",
        "\n",
        "        # Project QKV\n",
        "        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim)\n",
        "        k = self.k_proj(x).view(B, T, self.n_kv, self.head_dim)\n",
        "        v = self.v_proj(x).view(B, T, self.n_kv, self.head_dim)\n",
        "\n",
        "        # RoPE\n",
        "        cos, sin = self.rope(T, x.device, dtype)\n",
        "        cos, sin = cos[None, :, None, :], sin[None, :, None, :]\n",
        "        q = apply_rope(q, cos, sin)\n",
        "        k = apply_rope(k, cos, sin)\n",
        "\n",
        "        # Expand KV for grouped query\n",
        "        k = k.repeat_interleave(self.n_heads // self.n_kv, dim=2)\n",
        "        v = v.repeat_interleave(self.n_heads // self.n_kv, dim=2)\n",
        "\n",
        "        # Transpose for attention\n",
        "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
        "\n",
        "        # Attention scores\n",
        "        attn_scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # Causal mask\n",
        "        causal_mask = torch.tril(torch.ones(T, T, device=x.device, dtype=torch.bool))\n",
        "        attn_scores = attn_scores.masked_fill(~causal_mask, float(\"-inf\"))\n",
        "\n",
        "        # Softmax & output\n",
        "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "        out = attn_probs @ v\n",
        "        out = out.transpose(1,2).contiguous().view(B, T, C)\n",
        "        return self.o_proj(out)"
      ],
      "metadata": {
        "id": "oUfTaxNw-dex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfomer Blocks ‚Üí Llama 3.2 Model"
      ],
      "metadata": {
        "id": "tqfQ-DFh_yf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Transformer Block ---\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attn_norm = RMSNorm(config[\"emb_dim\"])\n",
        "        self.ffn_norm = RMSNorm(config[\"emb_dim\"])\n",
        "        self.attn = MultiHeadAttention(config)\n",
        "        self.ffn = FeedForward(config)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.attn_norm(x))\n",
        "        x = x + self.ffn(self.ffn_norm(x))\n",
        "        return x\n",
        "\n",
        "# =========================\n",
        "# 6Ô∏è‚É£ Llama32Model\n",
        "# =========================\n",
        "class Llama32Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(config[\"n_layers\"])])\n",
        "        self.norm = RMSNorm(config[\"emb_dim\"])\n",
        "        self.lm_head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
        "    def forward(self, input_ids):\n",
        "        x = self.embed(input_ids)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.norm(x)\n",
        "        return self.lm_head(x)"
      ],
      "metadata": {
        "id": "FAoekPrM_trJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test dummy input + Load the model on hardware"
      ],
      "metadata": {
        "id": "YGq3NZw2kW1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voPnu6mTl8Mh",
        "outputId": "916fbc6a-1865-4405-ea1d-301807833b2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = Llama32Model(LLAMA32_CONFIG).to(device).to(torch.bfloat16)\n",
        "dummy_input = torch.randint(\n",
        "    0,\n",
        "    LLAMA32_CONFIG[\"vocab_size\"],\n",
        "    (2, 128),\n",
        "    device=device\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(dummy_input)\n",
        "\n",
        "print(\"Output shape:\", logits.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juk85RxFkXh8",
        "outputId": "8cd927b7-5b84-4093-f4e1-1ee83a5c308d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 128, 128256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer from hugging face"
      ],
      "metadata": {
        "id": "3Q2sxifokb9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"  # or Llama-3.2-1B-Instruct\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    use_auth_token=\"\"\n",
        ")\n",
        "\n",
        "print(\"Tokenizer loaded!\")\n"
      ],
      "metadata": {
        "id": "xyS9g06kzPLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# encode a sample sentence into tokens and Decode back to text"
      ],
      "metadata": {
        "id": "pYYgb4raknOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello world! This is a test of the LLaMA 3.2 tokenizer.\"\n",
        "\n",
        "encoded = tokenizer(\n",
        "    text,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(\"Token IDs:\", encoded[\"input_ids\"])\n",
        "\n",
        "\n",
        "decoded = tokenizer.decode(\n",
        "    encoded[\"input_ids\"][0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(\"Decoded text:\", decoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fce0K2TBknt1",
        "outputId": "6ed5cc8c-86f6-40ec-ac70-ce56fc02dfaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs: tensor([[128000,   9906,   1917,      0,   1115,    374,    264,   1296,    315,\n",
            "            279,    445,   8921,   4940,    220,     18,     13,     17,  47058,\n",
            "             13]])\n",
            "Decoded text: Hello world! This is a test of the LLaMA 3.2 tokenizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tokenizer test for edge cases"
      ],
      "metadata": {
        "id": "PhwJfhJZk1Rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "TOKEN = \"\"\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    token=TOKEN\n",
        ")\n",
        "\n",
        "# üîë Explicitly set PAD = EOS (both token and id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# ---- Edge case batch test ----\n",
        "texts = [\"\", \" \", \"Hello\", \"üöÄ\"]\n",
        "\n",
        "encoded_batch = tokenizer(\n",
        "    texts,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "decoded_batch = [\n",
        "    tokenizer.decode(ids, skip_special_tokens=True)\n",
        "    for ids in encoded_batch[\"input_ids\"]\n",
        "]\n",
        "\n",
        "print(\"pad_token:\", tokenizer.pad_token)\n",
        "print(\"pad_token_id:\", tokenizer.pad_token_id)\n",
        "print(\"input_ids:\\n\", encoded_batch[\"input_ids\"])\n",
        "print(\"decoded:\", decoded_batch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3te129ukvAo",
        "outputId": "78dad808-fd8f-464b-d689-98e65a0b4362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pad_token: <|end_of_text|>\n",
            "pad_token_id: 128001\n",
            "input_ids:\n",
            " tensor([[128000, 128001, 128001, 128001],\n",
            "        [128000,    220, 128001, 128001],\n",
            "        [128000,   9906, 128001, 128001],\n",
            "        [128000,   9468,    248,    222]])\n",
            "decoded: ['', ' ', 'Hello', 'üöÄ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Access to the Llama 3.2 weights + simple test model prediction"
      ],
      "metadata": {
        "id": "eCOCngaPnUwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate --quiet\n"
      ],
      "metadata": {
        "id": "oDneKdgsmcdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "HF_TOKEN = \"\"  # Hugging Face token\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    use_auth_token=HF_TOKEN\n",
        ")\n",
        "\n",
        "# Ensure a pad token is defined for batch handling\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    use_auth_token=HF_TOKEN,\n",
        "    torch_dtype=torch.bfloat16  # use BF16 on GPUs that support it\n",
        ").to(device)\n",
        "\n",
        "model.eval()\n",
        "print(\"Model and tokenizer loaded!\")\n"
      ],
      "metadata": {
        "id": "rkP1-kVH0DjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model prediction"
      ],
      "metadata": {
        "id": "9lJCIEycqfWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Once upon a time,\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n"
      ],
      "metadata": {
        "id": "d2D3BAYvp-44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "    next_token_logits = logits[:, -1, :]  # last token\n",
        "    next_token_id = torch.argmax(next_token_logits, dim=-1)\n",
        "\n",
        "predicted_token = tokenizer.decode(next_token_id)\n",
        "print(\"Next token prediction:\", predicted_token)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0R7z-ozqBaA",
        "outputId": "2df67bdf-dc10-4a35-dd6e-aaaf3cd8c053"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next token prediction:  in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_tokens(prompt, max_new_tokens=10):\n",
        "    \"\"\"\n",
        "    Generate tokens iteratively using greedy decoding.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): Input text.\n",
        "        max_new_tokens (int): Number of tokens to predict.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated text (prompt + new tokens)\n",
        "    \"\"\"\n",
        "    # Encode prompt\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_ids).logits  # [batch, seq_len, vocab_size]\n",
        "            next_token_logits = logits[:, -1, :]  # last token logits\n",
        "            next_token_id = torch.argmax(next_token_logits, dim=-1)  # greedy\n",
        "        # Append predicted token\n",
        "        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "    # Decode the entire sequence\n",
        "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "    return generated_text\n"
      ],
      "metadata": {
        "id": "D3nS93KKqSow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Once upon a time,\"\n",
        "generated = predict_next_tokens(prompt, max_new_tokens=20)\n",
        "print(\"Generated text:\\n\", generated)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TboO12dXqVDc",
        "outputId": "0b12fc8c-808c-442f-f109-1a461b4f1c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            " Once upon a time, in a small village nestled in the rolling hills of the countryside, there lived a young girl named Sophia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare a small dataset"
      ],
      "metadata": {
        "id": "8A-QQ1hdrpCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class SmallDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=64):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.examples = []\n",
        "        for t in texts:\n",
        "            enc = tokenizer(t, truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "            self.examples.append(torch.tensor(enc[\"input_ids\"]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]\n",
        "\n",
        "# Sample dataset\n",
        "texts = [\n",
        "    \"Once upon a time, there was a brave knight.\",\n",
        "    \"The stock market showed significant gains today.\",\n",
        "    \"Machine learning can predict trends in healthcare.\",\n",
        "    \"Artificial intelligence is transforming science.\"\n",
        "]\n",
        "\n",
        "dataset = SmallDataset(texts, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
      ],
      "metadata": {
        "id": "wZs7RaQprQTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from torch.optim import AdamW   # <-- correct\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "jXAZODxks1KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HF_TOKEN = \"\"\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=HF_TOKEN)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    use_auth_token=HF_TOKEN,\n",
        "    torch_dtype=torch.bfloat16\n",
        ").to(device)\n",
        "\n",
        "model.train()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
      ],
      "metadata": {
        "id": "OiRgpgV80WEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(batch, labels=batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXsk-EW_tI5a",
        "outputId": "b0f2c491-9dab-48c1-b51c-84f6bddcd29c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Loss: 3.6103\n",
            "Epoch 2/20 - Loss: 0.2791\n",
            "Epoch 3/20 - Loss: 0.1399\n",
            "Epoch 4/20 - Loss: 0.1112\n",
            "Epoch 5/20 - Loss: 0.0895\n",
            "Epoch 6/20 - Loss: 0.0777\n",
            "Epoch 7/20 - Loss: 0.0679\n",
            "Epoch 8/20 - Loss: 0.0585\n",
            "Epoch 9/20 - Loss: 0.0494\n",
            "Epoch 10/20 - Loss: 0.0371\n",
            "Epoch 11/20 - Loss: 0.0309\n",
            "Epoch 12/20 - Loss: 0.0238\n",
            "Epoch 13/20 - Loss: 0.0258\n",
            "Epoch 14/20 - Loss: 0.0268\n",
            "Epoch 15/20 - Loss: 0.0264\n",
            "Epoch 16/20 - Loss: 0.0265\n",
            "Epoch 17/20 - Loss: 0.0266\n",
            "Epoch 18/20 - Loss: 0.0252\n",
            "Epoch 19/20 - Loss: 0.0254\n",
            "Epoch 20/20 - Loss: 0.0243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weights downloaded and loaded into model."
      ],
      "metadata": {
        "id": "mIsT1JXpt4OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub --quiet\n",
        "!huggingface-cli login\n"
      ],
      "metadata": {
        "id": "C-m2vf8OzIVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "repo_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "local_dir = \"./llama32_weights\"\n",
        "\n",
        "snapshot_download(repo_id, local_dir=local_dir, use_auth_token=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "a9229e296e174c2289bde58db5fcf7fa",
            "8037a3d902914e6496d0e386c4762449",
            "0616adf0f9bb4649b39bdb712a935152",
            "1f86cb20068e42b0b76f0148ce194ad9",
            "553d525d1f6a493981411095c326beaf",
            "11eba5ba0e0347f58d9532026fd73024",
            "3c2e9a7521744127b0039200c95a3753",
            "953e5e2714dc45699b26191c6d380592",
            "e5117941a313426f9a18ce91ade70ff4",
            "775288aa028c49ef8e7b5a086e05d8d4",
            "9eb86708fdd54bf89272a0f29b8a5f5a"
          ]
        },
        "id": "BopWGlZqu5Q7",
        "outputId": "712dd1eb-a640-4ba6-9dbd-ac7be294c0ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9229e296e174c2289bde58db5fcf7fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/llama32_weights'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load tokenizer locally\n",
        "tokenizer = AutoTokenizer.from_pretrained(local_dir)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model locally\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    local_dir,\n",
        "    torch_dtype=torch.bfloat16\n",
        ").to(device)\n",
        "\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500,
          "referenced_widgets": [
            "7dc19afc174540b8ae6d36f4f7f736e3",
            "4c3494e3330646e18fbfff48c7640645",
            "7bf7055b8d96476b9dce8dbbd369cef5",
            "ac3313c7a39940f19b11cbcf5246accd",
            "2c3fae2aa48342dea333dae5769e44c0",
            "d621d4abf6794eb08381c4fcdfcf9a63",
            "394b3b3ac07a4423ad583a1076a1bda0",
            "1f3965854c11497490958bd1192ee3f7",
            "296ba4a3d4bf454d8557ef98ebeeff7f",
            "1ab0d196ba12488490cd19bdf6d9c946",
            "399b3747d71641e98b923bbefd37dd58"
          ]
        },
        "id": "UJ6Cf5ifvJhq",
        "outputId": "252141fa-a4ec-4126-aae4-c9071272298e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7dc19afc174540b8ae6d36f4f7f736e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 3072)\n",
              "    (layers): ModuleList(\n",
              "      (0-27): 28 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
              "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
              "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
              "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
              "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
              "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Once upon a time,\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    next_token_id = torch.argmax(outputs.logits[:, -1, :], dim=-1)\n",
        "\n",
        "print(\"Next token:\", tokenizer.decode(next_token_id))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CclTykubvN8v",
        "outputId": "29ae71f3-9319-4459-b242-5bd61e2c8ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next token:  in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_tokens(prompt, max_new_tokens=10):\n",
        "    \"\"\"\n",
        "    Generate tokens iteratively using greedy decoding.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): Input text.\n",
        "        max_new_tokens (int): Number of tokens to predict.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated text (prompt + new tokens)\n",
        "    \"\"\"\n",
        "    # Encode prompt\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_ids).logits  # [batch, seq_len, vocab_size]\n",
        "            next_token_logits = logits[:, -1, :]  # last token logits\n",
        "            next_token_id = torch.argmax(next_token_logits, dim=-1)  # greedy\n",
        "        # Append predicted token\n",
        "        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "    # Decode the entire sequence\n",
        "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "    return generated_text\n"
      ],
      "metadata": {
        "id": "P_FUtLqBvbnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Once upon a time,\"\n",
        "generated = predict_next_tokens(prompt, max_new_tokens=20)\n",
        "print(\"Generated text:\\n\", generated)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LfPcCe0vh2w",
        "outputId": "858aaae2-7796-402b-8b07-ce7adbbae48b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            " Once upon a time, in a small village nestled in the rolling hills of the countryside, there lived a young girl named Sophia\n"
          ]
        }
      ]
    }
  ]
}